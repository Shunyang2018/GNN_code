{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch run for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "# Define the list of models to test\n",
    "models = ['GCN', 'GAT', 'Weave', 'MPNN', 'AttentiveFP',\n",
    "          'gin_supervised_contextpred', 'gin_supervised_infomax',\n",
    "          'gin_supervised_edgepred', 'gin_supervised_masking', 'NF']\n",
    "\n",
    "# Base command template\n",
    "base_command = (\n",
    "    \"python regression_train.py \"\n",
    "    \"-c ./C18RT.csv \"\n",
    "    \"-sc 'CanonicalSMILES' -nw 6 -me 'mae' -p './results' \"\n",
    "    \"-mo '{model}' -t 'C18RT' -s random -sr 0.8,0.1,0.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: GCN\n",
      "Training model: GAT\n",
      "Training model: Weave\n",
      "Training model: MPNN\n",
      "Training model: AttentiveFP\n",
      "Training model: gin_supervised_contextpred\n",
      "Training model: gin_supervised_infomax\n",
      "Training model: gin_supervised_edgepred\n",
      "Training model: gin_supervised_masking\n",
      "Training model: NF\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "for model in models:\n",
    "    print(f\"Training model: {model}\")\n",
    "\n",
    "    # Execute the command\n",
    "    try:\n",
    "        # Load the evaluation results\n",
    "        eval_path = f\"./results/{model}/eval.txt\"\n",
    "        if os.path.exists(eval_path):\n",
    "            with open(eval_path, 'r') as f:\n",
    "                eval_results = f.readlines()\n",
    "\n",
    "            # Extract metrics from the evaluation file\n",
    "            metrics = {}\n",
    "            for line in eval_results:\n",
    "                if \":\" in line:\n",
    "                    key, value = line.split(\":\")\n",
    "                    metrics[key.strip()] = float(value.strip())\n",
    "\n",
    "            results[model] = metrics\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to train model {model}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for all models:\n",
      "GCN: {'Best val mae': 0.49150320887565613, 'Test mae': 0.45785361528396606}\n",
      "GAT: {'Best val mae': 0.3472808003425598, 'Test mae': 0.3785816431045532}\n",
      "Weave: {'Best val mae': 0.4747655391693115, 'Test mae': 0.48232242465019226}\n",
      "MPNN: {'Best val mae': 0.3982100188732147, 'Test mae': 0.4415026307106018}\n",
      "AttentiveFP: {'Best val mae': 0.3214002549648285, 'Test mae': 0.33426281809806824}\n",
      "gin_supervised_contextpred: {'Best val mae': 0.4089275598526001, 'Test mae': 0.4010923206806183}\n",
      "gin_supervised_infomax: {'Best val mae': 0.4564848244190216, 'Test mae': 0.4705674350261688}\n",
      "gin_supervised_edgepred: {'Best val mae': 0.439164400100708, 'Test mae': 0.41336357593536377}\n",
      "gin_supervised_masking: {'Best val mae': 0.45019441843032837, 'Test mae': 0.45540446043014526}\n",
      "NF: {'Best val mae': 1.2017685174942017, 'Test mae': 1.2223578691482544}\n",
      "\n",
      "Best model: AttentiveFP with Test mae: 0.33426281809806824\n"
     ]
    }
   ],
   "source": [
    "# Determine the best model based on the chosen metric (e.g., lowest MAE)\n",
    "metric_to_optimize = 'Test mae'\n",
    "best_model = min(results, key=lambda x: results[x][metric_to_optimize])\n",
    "\n",
    "# Output the results\n",
    "print(\"Results for all models:\")\n",
    "for model, metrics in results.items():\n",
    "    print(f\"{model}: {metrics}\")\n",
    "\n",
    "print(f\"\\nBest model: {best_model} with {metric_to_optimize}: {results[best_model][metric_to_optimize]}\")\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open(\"all_model_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogluon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
